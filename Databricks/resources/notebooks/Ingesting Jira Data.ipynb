{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b246b3c-23fc-4dad-b481-0f1a49d4e47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Retrieve data from g-sheet by using Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b3c24b-edc8-44e7-8dbf-7da468acad75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gspread\n",
      "  Downloading gspread-6.2.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gspread_dataframe\n",
      "  Downloading gspread_dataframe-4.0.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: google-auth>=1.12.0 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from gspread) (2.47.0)\n",
      "Collecting google-auth-oauthlib>=0.4.1 (from gspread)\n",
      "  Downloading google_auth_oauthlib-1.2.4-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from gspread_dataframe) (2.3.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from gspread_dataframe) (1.17.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from google-auth>=1.12.0->gspread) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from google-auth>=1.12.0->gspread) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=1.12.0->gspread) (0.6.1)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib>=0.4.1->gspread)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from pandas>=0.24.0->gspread_dataframe) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from pandas>=0.24.0->gspread_dataframe) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from pandas>=0.24.0->gspread_dataframe) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from pandas>=0.24.0->gspread_dataframe) (2025.3)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/daniel/Desktop/DE_Project/Effort-Tracking-Analytics-Engineer-project/.venv/lib/python3.11/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2026.1.4)\n",
      "Downloading gspread-6.2.1-py3-none-any.whl (59 kB)\n",
      "Downloading gspread_dataframe-4.0.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading google_auth_oauthlib-1.2.4-py3-none-any.whl (19 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, google-auth-oauthlib, gspread, gspread_dataframe\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [gspread_dataframe]\n",
      "\u001b[1A\u001b[2KSuccessfully installed google-auth-oauthlib-1.2.4 gspread-6.2.1 gspread_dataframe-4.0.0 oauthlib-3.3.1 requests-oauthlib-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gspread gspread_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdaa905a-a5fd-42c8-94b5-bf9b85b4b080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step1: Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb0502c-235a-432f-8e1b-f35c533b6910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 7195 rows\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "#config Variable\n",
    "SPREADSHEET_ID = '1iWmtI5iHzgxA9S9HdXloq7ak8VHA5mYO01f1MMaZ5-A'\n",
    "SHEET_NAME ='ticket'\n",
    "#Using Databricks secrets to access the service account key for security\n",
    "SERVICE_ACCOUNT_JSON = dbutils.secrets.get(scope=\"ingestdata\", key=\"service-account-key\")\n",
    "service_account_dict = json.loads(SERVICE_ACCOUNT_JSON)\n",
    "\n",
    "\n",
    "gc = gspread.service_account_from_dict(service_account_dict)\n",
    "sh = gc.open_by_key(SPREADSHEET_ID)\n",
    "ws = sh.worksheet(SHEET_NAME)\n",
    "\n",
    "# Extract to DataFrame\n",
    "df = get_as_dataframe(ws, parse_date=True, evaluate_formulas=True)\n",
    "\n",
    "#Convert to Spark, add metadata\n",
    "sheet_df = spark.createDataFrame(df)\n",
    "sheet_df = sheet_df.withColumn('Extracted At', current_timestamp())\n",
    "\n",
    "# Validate extracted data\n",
    "assert not sheet_df.count() < 0, \"Extracted data is empty\"\n",
    "\n",
    "print(f\"Extracted {sheet_df.count()} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29657e8f-0306-47d7-82e5-350a782ca5d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Cleaning Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46076bc-36c1-4c6d-9927-deeb330ae5a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sanitize column names: replace spaces and special characters with underscores\n",
    "def sanitize_column(col_name):\n",
    "    return (\n",
    "        col_name.replace(\" \", \"_\")\n",
    "        .replace(\".\", \"_\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "        .replace(\"`\", \"\")\n",
    "        .replace(\",\", \"\")\n",
    "        .replace(\";\", \"\")\n",
    "        .replace(\"{\", \"\")\n",
    "        .replace(\"}\", \"\")\n",
    "        .replace(\"\\n\", \"\")\n",
    "        .replace(\"\\t\", \"\")\n",
    "        .replace(\"=\", \"_\")\n",
    "    )\n",
    "\n",
    "raw_jira_data_clean = sheet_df.toDF(\n",
    "    *[sanitize_column(col) for col in sheet_df.columns]\n",
    ")\n",
    "\n",
    "raw_jira_data_clean.createOrReplaceTempView(\"raw_jira_data_clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "476b6027-6315-40b5-86f4-e8bd8e012b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Loading Raw Data to SCR_JIRA table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e765db2-728b-4773-bc51-16029ddcb276",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Key\":549},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768275141388}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "TARGET_TABLE = \"looker_management_prod.bronze.scr_jira\"\n",
    "LOAD_DATE = date.today().isoformat()\n",
    "\n",
    "##CREATE TABLE IF NOT EXISTS (infers schema from source)\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TARGET_TABLE} \n",
    "        USING DELTA\n",
    "        COMMENT 'Jira Data - Weekly Load'\n",
    "        TBLPROPERTIES (\n",
    "            delta.autoOptimize.optimizeWrite = true,\n",
    "            delta.autoOptimize.autoCompact = true\n",
    "        )\n",
    "        AS SELECT * FROM raw_jira_data_clean\n",
    "\"\"\")\n",
    "\n",
    "#MERGE (UPSERT)\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {TARGET_TABLE} AS t\n",
    "    USING (\n",
    "        SELECT * FROM raw_jira_data_clean\n",
    "    ) s\n",
    "    ON t.key = s.key\n",
    "    AND t.Updated = s.Updated\n",
    "    WHEN MATCHED THEN \n",
    "      UPDATE SET *\n",
    "    \n",
    "    WHEN NOT MATCHED THEN \n",
    "      INSERT *\n",
    "          \"\"\")\n",
    "\n",
    "print(f\"Merged {spark.table(TARGET_TABLE).count()} rows\")\n",
    "display(spark.table(TARGET_TABLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f52e51-808d-4f40-bf10-f5984a574cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Step 4: Loading data to SCR_JIRA_TRNS\n",
    "Unpivot Sprint column in SCR_JIRA Table and load to SCR_JIRA_TRNS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c7ca2d8-06ba-4be8-9789-9f41374cc20b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Step 4: Unpivot Sprint column in SCR_JIRA Table"
    }
   },
   "outputs": [],
   "source": [
    "TARGET_TABLE = \"looker_management_prod.bronze.scr_jira_trns\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {TARGET_TABLE} \n",
    "USING DELTA\n",
    "COMMENT 'Transformed Jira Data'\n",
    "TBLPROPERTIES (\n",
    "    delta.autoOptimize.optimizeWrite = true,\n",
    "    delta.autoOptimize.autoCompact = true\n",
    ")\n",
    "AS\n",
    "WITH JIRA_TRANSF_01 AS (\n",
    "    SELECT \n",
    "        CAST(Issue_Type AS STRING) AS TCK_TP,\n",
    "        CAST(Summary AS STRING) AS TCK_NM,\n",
    "        CAST(Assignee AS STRING) AS ASN_NM,\n",
    "        CAST(Status AS STRING) AS TCK_STS,\n",
    "        CAST(Sprint AS STRING) AS PI_ID,\n",
    "        CAST(Est__Story_Points AS FLOAT) AS STR_PNT,\n",
    "        CAST(`Key` AS STRING) AS TCK_ID,\n",
    "        CAST(parent AS STRING) AS PRN_ID,\n",
    "        TRY_CAST(Start_date AS DATE) AS STR_DT,\n",
    "        TRY_CAST(End_date AS TIMESTAMP) AS END_DT,\n",
    "        TRY_CAST(Updated AS TIMESTAMP) AS UPD_DT,\n",
    "        CAST(Assignee_accountId AS STRING) AS DEV_ID\n",
    "    FROM looker_management_prod.bronze.scr_jira\n",
    "),\n",
    "JIRA_TRNSF_02 AS (\n",
    "    SELECT\n",
    "        TCK_ID,\n",
    "        COALESCE(PRN_ID,'Undefined')  AS PRN_ID,\n",
    "        COALESCE(DEV_ID,'Undefined')  AS DEV_ID,\n",
    "        COALESCE(PI_ID,'Undefined')   AS PI_ID,\n",
    "        COALESCE(ASN_NM,'Undefined')  AS ASN_NM,\n",
    "        COALESCE(TCK_NM,'Undefined')  AS TCK_NM,\n",
    "        COALESCE(TCK_TP,'Undefined')  AS TCK_TP,\n",
    "        COALESCE(TCK_STS,'Undefined') AS TCK_STS,\n",
    "        COALESCE(STR_PNT,0)           AS STR_PNT,\n",
    "        COALESCE(STR_DT,'1900-01-01') AS STR_DT,\n",
    "        COALESCE(END_DT,'1900-01-01') AS END_DT,\n",
    "        COALESCE(UPD_DT,'1900-01-01') AS UPD_DT\n",
    "    FROM JIRA_TRANSF_01\n",
    "),\n",
    "JIRA_TRNSF_03 AS (\n",
    "    SELECT \n",
    "        TCK_ID,\n",
    "        PRN_ID,\n",
    "        DEV_ID,\n",
    "        PI_ID_EXPLODED AS PI_ID,\n",
    "        ASN_NM,\n",
    "        TCK_NM,\n",
    "        TCK_TP,\n",
    "        TCK_STS,\n",
    "        STR_PNT,\n",
    "        STR_DT,\n",
    "        END_DT,\n",
    "        UPD_DT\n",
    "    FROM JIRA_TRNSF_02\n",
    "    LATERAL VIEW explode(split(PI_ID, ';')) AS PI_ID_EXPLODED\n",
    ")\n",
    "SELECT\n",
    "    TCK_ID,\n",
    "    PRN_ID,\n",
    "    DEV_ID,\n",
    "    PI_ID,\n",
    "    ASN_NM,\n",
    "    TCK_NM,\n",
    "    TCK_TP,\n",
    "    TCK_STS,\n",
    "    STR_PNT,\n",
    "    STR_DT,\n",
    "    END_DT,\n",
    "    UPD_DT\n",
    "FROM JIRA_TRNSF_03\n",
    "WHERE TCK_ID IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711835dd-3118-456e-8511-81100ff1d4e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesting Jira Data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
